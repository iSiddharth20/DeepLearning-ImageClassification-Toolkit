{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0e5186",
   "metadata": {},
   "source": [
    "Source : https://github.com/iSiddharth20/DeepLearning-ImageClassification-Toolkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4638d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Working Directories\n",
    "Name of Model\n",
    "'''\n",
    "import os\n",
    "\n",
    "# Directory where Pickle Files will be Stored\n",
    "PICKLE_DIR = '../PickleFiles/'\n",
    "\n",
    "# Directory where Models will be Stored (Created by Code)\n",
    "MODEL_DIR = '../TrainedModels/'\n",
    "os.makedirs(os.path.dirname(MODEL_DIR), exist_ok=True)\n",
    "\n",
    "# Name of Model Used\n",
    "MODEL_NAME = 'EfficientNetB0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1f5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing Necessary Libraries and Packages\n",
    "'''\n",
    "# Disable TensorFlow Warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "# Helpers\n",
    "import pickle\n",
    "\n",
    "# To log Trainnig Time\n",
    "from datetime import datetime\n",
    "\n",
    "# Installing EfficientNet\n",
    "! pip install -U git+https://github.com/qubvel/efficientnet\n",
    "\n",
    "# Importing TensorFlow and EfficientNet\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import Precision, Recall, AUC\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import efficientnet.tfkeras as efn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ddf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Importing the following One-Hot-Encoded Data from Pickle Files\n",
    "    - ClassLabels_train\n",
    "    - ClassLabels_test\n",
    "    - ClassLabels_validation\n",
    "Importing the following from Pickle Files\n",
    "    - ImgArray_train\n",
    "    - ImgArray_test\n",
    "    - ImgArray_validation\n",
    "Importing One-Hot-Encoding\n",
    "'''\n",
    "\n",
    "# Importing ClassLabels_train\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ClassLabels_train.pkl', 'rb') as f: ClassLabels_train = pickle.load(f)\n",
    "    print('Import Successful for : ClassLabels_train.pkl ')  \n",
    "except:\n",
    "    print('Import Unsuccessful for : ClassLabels_train.pkl ')\n",
    "\n",
    "# Importing ClassLabels_test\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ClassLabels_test.pkl', 'rb') as f: ClassLabels_test = pickle.load(f)\n",
    "    print('Import Successful for : ClassLabels_test.pkl ')  \n",
    "except:\n",
    "    print('Import Unsuccessful for : ClassLabels_test.pkl ')\n",
    "\n",
    "# Importing ClassLabels_validation\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ClassLabels_validation.pkl', 'rb') as f: ClassLabels_validation = pickle.load(f)\n",
    "    print('Import Successful for : ClassLabels_validation.pkl ')  \n",
    "except:\n",
    "    print('Import Unsuccessful for : ClassLabels_validation.pkl ')\n",
    "\n",
    "    \n",
    "# Importing ImgArray_train\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ImgArray_train.pkl', 'rb') as f: ImgArray_train = pickle.load(f)\n",
    "    print('Import Successful for : ImgArray_train.pkl')  \n",
    "except:\n",
    "    print('Import Unsuccessful for : ImgArray_train.pkl ')\n",
    "\n",
    "# Importing ImgArray_test\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ImgArray_test.pkl', 'rb') as f: ImgArray_test = pickle.load(f)\n",
    "    print('Import Successful for : ImgArray_test.pkl')\n",
    "except:\n",
    "    print('Import Unsuccessful for : ImgArray_test.pkl ') \n",
    "\n",
    "# Importing ImgArray_validation\n",
    "try:\n",
    "    with open(PICKLE_DIR+'ImgArray_validation.pkl', 'rb') as f: ImgArray_validation = pickle.load(f)\n",
    "    print('Import Successful for : ImgArray_validation.pkl')\n",
    "except:\n",
    "    print('Import Unsuccessful for : ImgArray_validation.pkl ') \n",
    "\n",
    "    \n",
    "# Importing One-Hot-Encoding \n",
    "try:\n",
    "    with open(PICKLE_DIR+'OHE.pkl', 'rb') as f: OHE = pickle.load(f)\n",
    "    print('Import Successful for : OHE.pkl')\n",
    "except:\n",
    "    print('Import Unsuccessful for : OHE.pkl ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea3aaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preparing Data Pipeline\n",
    "    - Convert All Data to TensorFlow Format\n",
    "    - Defining Data Augmentation Function\n",
    "    - Defining Rescaled Image Height, Width\n",
    "'''\n",
    "\n",
    "BATCH_SIZE = 12 # Change as per Requirement\n",
    "IMG_HEIGHT = 400\n",
    "IMG_WIDTH = 600\n",
    "IMG_CHANNELS = 3 # Assuming RGB Image\n",
    "\n",
    "# Training Dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((ImgArray_train,ClassLabels_train))\n",
    "train_dataset = train_dataset.shuffle(10000, reshuffle_each_iteration=True)\n",
    "train_dataset = train_dataset.cache()\n",
    "train_dataset = train_dataset.repeat()\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "del(ImgArray_train) # Clear RAM\n",
    "\n",
    "# Testing Dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((ImgArray_test,ClassLabels_test))\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)\n",
    "del(ImgArray_test) # Clear RAM\n",
    "del(ClassLabels_test) # Clear RAM\n",
    "\n",
    "# Validation Dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((ImgArray_validation,ClassLabels_validation))\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE)\n",
    "del(ImgArray_validation) # Clear RAM\n",
    "\n",
    "# Data Augmentation\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomTranslation(0.03, 0.06),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.015),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomHeight(0.1, interpolation='nearest'),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomWidth(0.1), \n",
    "])\n",
    "\n",
    "# Steps Per Epoch and Validation Steps\n",
    "steps_per_epoch = len(ClassLabels_train) // BATCH_SIZE\n",
    "validation_steps = len(ClassLabels_validation) // BATCH_SIZE\n",
    "del(ClassLabels_train) # Clear RAM\n",
    "del(ClassLabels_validation) # Clear RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d76b9-3a0b-4965-a9ef-29238227a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "earlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_auc', patience=10, verbose=2)\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(MODEL_DIR+MODEL_NAME+'.h5', save_best_only=True ,verbose=1)\n",
    "\n",
    "# Learning Rate Scheduler (Change as per Requirement)\n",
    "initial_learning_rate = 0.01 \n",
    "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=8, verbose=1, mode='min', cooldown=1, min_lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc23e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Building Training Model [EfficientNetB0]\n",
    "    - Load Base Model\n",
    "    - Build Final Model\n",
    "    - Compile Final Model\n",
    "'''\n",
    "\n",
    "# Loading Base Model\n",
    "OHE_classes = OHE.categories_[0]\n",
    "num_classes = len(OHE_classes)\n",
    "del(OHE) # Clear RAM\n",
    "DROP_CONNECT = 0.15\n",
    "\n",
    "# Create Model which can Leverage All Avavilable GPUs \n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    \n",
    "    # Load Base Model\n",
    "    base_model = efn.EfficientNetB0(weights='imagenet', include_top=False,\n",
    "                               input_shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS),\n",
    "                               drop_connect_rate = DROP_CONNECT)\n",
    "    \n",
    "    # Building Final Model\n",
    "    inputs = tf.keras.layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    x = data_augmentation(inputs)\n",
    "    x = tf.keras.layers.experimental.preprocessing.Rescaling(1.0/255)(x)\n",
    "    x = base_model(x)\n",
    "    del(base_model) # Clear RAM\n",
    "    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "    outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    del(x) # Clear RAM\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    \n",
    "    # Compiling Final Model\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=initial_learning_rate, momentum=0.9),\n",
    "        metrics=['accuracy', Precision(name='precision'), Recall(name='recall'), AUC(name='auc')])\n",
    "\n",
    "# View the Model Summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4c4b0d-752a-4ca4-94f8-b1ff792a5ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Re-Training Trained Model [EfficientNetB0]\n",
    "    - Uncomment when Necessary\n",
    "'''\n",
    "\n",
    "# # Loading Model\n",
    "# model = tf.keras.models.load_model(MODEL_DIR+MODEL_NAME+'.h5')\n",
    "\n",
    "# # View the Model Summary\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269629f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train the Model\n",
    "'''\n",
    "begin = datetime.now()\n",
    "\n",
    "EPOCHS = 50 # Change as per Requirement\n",
    "history = model.fit(train_dataset, validation_data=val_dataset, \n",
    "                    steps_per_epoch=steps_per_epoch, validation_steps=validation_steps, \n",
    "                    epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=1, \n",
    "                    callbacks = [checkpointer, earlystopper, lr_scheduler])\n",
    "\n",
    "finish = datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81532146-7b1c-4f66-b473-aca66bcfaeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Display Time Required (In Seconds) to Train the Model\n",
    "'''\n",
    "\n",
    "total_time = finish-begin\n",
    "print('Total Training Time (In Seconds) for '+MODEL_NAME+' : ',total_time.total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluate the Model\n",
    "'''\n",
    "\n",
    "model = tf.keras.models.load_model(MODEL_DIR+MODEL_NAME+'.h5')\n",
    "\n",
    "print('Training : ', model.evaluate(train_dataset, steps=steps_per_epoch))\n",
    "print('Validation : ', model.evaluate(val_dataset))\n",
    "print('Testing : ', model.evaluate(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbfc63d-d6bf-4ad7-952f-f26d75295f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
